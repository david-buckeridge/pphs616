---
title: "Module 2 - Starter Code"
author: "David Buckeridge"
date: "January 19, 2021"
output: html_document
---

```{r setup, include=F, message=F}
# Load librairies HERE
library(tidyverse)
library("knitr")
library("surveillance")
library("MESS")

# Note: The code in this week's assignment is resource intensive, you might want to work on this
#       assignment on a powerful computer. (You can also work with smaller sets while debugging.)

# Set your working directory as needed.
opts_knit$set(root.dir = "C:/src/pphs616/")

# define locations of data and key file mapping run ids to simulation scenarios
data.dir = "data/surveillance_subset_noBWA_100samples"
key.filename = "data/key.csv"
```

## 1. Using the surveillance package to simulate and test

### Comparing the EARS methods

```{r ears}
# Load helper functions
source("functions/outbreak.functions.R")

# simulate many time series - use for all
set.seed(1985)

# simulate a time series
one.sts = sim.pointSource(p = 0.99, r = 0.5, length = 400,
                       A = 1, alpha = 1, beta = 0, phi = 0,
                       frequency = 1, state = NULL, K = 1.7)

# simulate many time series
many = 200
many.sts = lapply(1:many, function(x) {
                  sim.pointSource(p = 0.99, r = 0.5, length = 400,
                                  A = 1, alpha = 1, beta = 0, phi = 0,
                                  frequency = 1, state = NULL, K = 1.7)})

plot(one.sts)
```

```{r ears_c1}
# create algorithm control object
C1.control = list(
  method = "C1",
  baseline = 7,
  alpha = 0.001,
  range=c(100:400))

# apply C1 algorithm to one sts
C1.one = earsC(disProg2sts(one.sts), control = C1.control)

# apply C1 algorith to many sts
C1.many = lapply(many.sts, function(ts) {
  earsC(disProg2sts(ts), control=C1.control)})

# plot results for single application
plot(C1.one)

# assess performance for single application
algo.quality(C1.one)

# assess performance
C1.many.quality = as.data.frame(algo.compare(C1.many))

C1.many.se = sum(unlist(C1.many.quality$TP)) / sum(unlist(C1.many.quality$TP) + unlist(C1.many.quality$FN))
C1.many.sp = sum(unlist(C1.many.quality$TN)) / sum(unlist(C1.many.quality$TN) + unlist(C1.many.quality$FP))
```

```{r ears_c2}
# 1. Create algorithm control object
C2.control = list(
  method = "C2",
  baseline = 7,
  alpha = 0.001,
  range=c(100:400))

# 2a. Apply C2 algorithm to one sts 
C2.one = earsC(disProg2sts(one.sts), control = C2.control)
# 2b. Apply C2 algorith to many sts
C2.many = lapply(many.sts, function(ts) {
  earsC(disProg2sts(ts), control=C2.control)})

# 3a. Plot results for single application
plot(C2.one)
# 3b. Assess performance for single application
C2.qual <- algo.quality(C2.one)

# 4. Assess performance
C2.many.quality = as.data.frame(algo.compare(C2.many))
```

```{r ears_c3}
# 1. Create algorithm control object
C3.control = list(
  method = "C3",
  baseline = 7,
  alpha = 0.001,
  range=c(100:400))

# 2a. Apply C3 algorithm to one sts 
C3.one = earsC(disProg2sts(one.sts), control = C3.control)
# 2b. Apply C3 algorith to many sts
C3.many = lapply(many.sts, function(ts) {
  earsC(disProg2sts(ts), control=C3.control)})

# 3a. Plot results for single application
plot(C3.one)
# 3b. Assess performance for single application
C3.qual <- algo.quality(C3.one)

# 4. Assess performance
C3.many.quality = as.data.frame(algo.compare(C3.many))
```


**Q1. Compare the performance of C1, C2, and C3 using the default settings.**

```{r q1}
## Apply all methods with default settings to same simulated outbreaks. Calculate 
##  overall sensitivity and specificty for each method. Which is better and why? Comment 
##  on how sensitivity is calculated.

#Table of Sensitivities & Specificities

#1. Summary Calculations
C1.many.se = sum(unlist(C1.many.quality$TP)) / sum(unlist(C1.many.quality$TP) + unlist(C1.many.quality$FN))
C1.many.sp = sum(unlist(C1.many.quality$TN)) / sum(unlist(C1.many.quality$TN) + unlist(C1.many.quality$FP))
C1.many.mlag = mean(unlist(C1.many.quality$mlag))

C2.many.se = sum(unlist(C2.many.quality$TP)) / sum(unlist(C2.many.quality$TP) + unlist(C2.many.quality$FN))
C2.many.sp = sum(unlist(C2.many.quality$TN)) / sum(unlist(C2.many.quality$TN) + unlist(C2.many.quality$FP))
C2.many.mlag = mean(unlist(C2.many.quality$mlag))

C3.many.se = sum(unlist(C3.many.quality$TP)) / sum(unlist(C3.many.quality$TP) + unlist(C3.many.quality$FN))
C3.many.sp = sum(unlist(C3.many.quality$TN)) / sum(unlist(C3.many.quality$TN) + unlist(C3.many.quality$FP))
C3.many.mlag = mean(unlist(C3.many.quality$mlag))

#2. Insert into dataframe
ears.dat <- data.frame(method=c("C1", "C2", "C3"), sens=c(C1.many.se, C2.many.se, C3.many.se), 
                       spec=c(C1.many.sp, C2.many.sp, C3.many.sp), 
                       mlag=c(C1.many.mlag, C2.many.mlag, C3.many.mlag))

knitr::kable(ears.dat, title = "Table 1: Summary of EARS Method Performances", col.names = c("Method", "Sensitivity", "Specificity", "Average of Median Lag Times"), align = 'c', digits = c(0,2,2,2))
```


### Finding the best parameters for the Farrington method

```{r farrington}

# Declare algorithms to apply and set their parameters 
F.control = list(
  list(funcName = 'farrington', alpha=1.5),
  list(funcName = 'farrington', alpha=1.25),
  list(funcName = 'farrington', alpha=0.75),
  list(funcName = 'farrington', alpha=0.5),
  list(funcName = 'farrington', alpha=0.25),
  list(funcName = 'farrington', alpha=0.2),
  list(funcName = 'farrington', alpha=0.15),
  list(funcName = 'farrington', alpha=0.1),
  list(funcName = "farrington", alpha=0.075),
  list(funcName = "farrington", alpha=0.05),
  list(funcName = "farrington", alpha=0.04),
  list(funcName = "farrington", alpha=0.03),
  list(funcName = "farrington", alpha=0.025),
  list(funcName = 'farrington', alpha=0.01),
  list(funcName = 'farrington', alpha=0.0075),
  list(funcName = 'farrington', alpha=0.005),
  list(funcName = 'farrington', alpha=0.0025),
  list(funcName = 'farrington', alpha=0.001),
  list(funcName = 'farrington', alpha=0.0005),
  list(funcName = 'farrington', alpha=0.00025),
  list(funcName = 'farrington', alpha=0.000005),
  list(funcName = 'farrington', alpha=0.00000000005))


# Define interval in sts for surveillance. Note that you need to have sufficient
#  "lead time" to apply the Farrington algorithm
F.control = lapply(F.control, function(ctrl) {
  ctrl$range = 300:400; return(ctrl)})


# apply to all simulated series, with results as list
F.many = lapply(many.sts, function(ts) {
  algo.compare(algo.call(ts, control=F.control)) })


#Average results
F.many.quality = algo.summary(F.many)
```

**Q2. Generate the necessary results and plot an ROC curve using at least ten points (not counting the origins). Which threshold do you recommend be used and why?**

```{r q2}
# Place any additional code here
F.many.quality = as.data.frame(algo.summary(F.many))
F.many.quality <- F.many.quality %>% 
  mutate(F.alert = (1-spec))
ggplot(F.many.quality, aes(x=F.alert, y=sens)) +
  geom_line() +
  geom_point() +
  labs(title = "Receiver Operating Curve for Simulations of Farrington Algorithm under Multiple Conditions", caption="Sensitivity is the proportion of alarms during an outbreak; Specificity is calculated as the proportion of 'non-alarms' during an outbreak", 
       x="False Alert Rate (1-Specificity)", y="Sensitivity", colour = 'dark red') +
  xlim(0, 0.35) +
  ylim(0, 1.0)
```

According to this figure, an alpha of approximately 0.04 would provide the best balance between sensitivity and the false alarm rate. At this alpha, the sensitivity is ~ 0.945 or 94.5%, which implies that the algorithm would report a high proportion of aberrations during the time there is an outbreak. However the confidence interval that is generated by the forecast is fairly wide, so it will capture more observations within its bounds and as a result, consider fewer of them to be outbreaks; this is reducing the false alarm rate to approximately 0.038, or 3.8%. As the false alarm rate is very low but the sensitivity is also high, this alpha will provide the optimal detection threshold for these data.

## 2. Using base R commands with externally simuated outbreaks

### Read Simulated Outbreaks

```{r read_outbreaks}
# Set this number low for initial attempts, then use all the runs (at the indicated
#  concentration and duration) to answer the questions.
nruns = 100

# Generate n (1 to 100) runids for scenario with concentration 0.1 and duration 24 hours
runids = get.runids(key.filename, concentration=0.01, duration=72, n=nruns)

# If you want to use the same sample of runs each time, save the runids and then reload
#  them again, as opposed to generating new ids

#write(runids,"runids.txt")
#runids = (read.table("runids.txt"))[,1]

# load runs corresponding to runids
# runs = load.runs(data.dir, runids)
runs = load.runs(data.dir, runids)
```

### Describe Outbreaks

```{r outbreaks}
# Calculate summary outbreak information and truth vectors for runs
outbreaks = lapply(runs, o.summary)

# Plot distribution of outbreak by maximum height and duration
par(mfrow=c(1,2))
hist(unlist(sapply(outbreaks, "[", "height")), xlab="Maximum Height (Daily Visits)", main="Maximum Height")
hist(unlist(sapply(outbreaks, "[", "length")), xlab="Duration (Days)", main="Duration")
par(mfrow=c(1,1))
```

### Apply Methods to Simulated Daily Time Series

```{r methods}
# Number of thresholds to consider when generating ROC curves
n.cutoffs = 100
```

### Apply C2 Algorithm

```{r c2_algo}
# Apply C2 algorithm to runs
res.c2 = lapply(runs, c2_all, gap=2, window=28, threshold=2)
# Determine detection and timeliness for each run
res.c2.detect = mapply(o.detected, res.c2, outbreaks)
res.c2.prevent = mapply(o.prevented, res.c2, outbreaks)

# Calculate accuracy and timeliness for each run
performance.c2.all = a.performance.all(res.c2, outbreaks, n.cutoffs)
# Calculate average accuracy and timeliness acros all runs
performance.c2.avg = a.performance.avg(performance.c2.all)
# Calculate area under ROC curves
auc.c2 = auc(performance.c2.avg$far, performance.c2.avg$detected)
auc.c2.weighted = auc(performance.c2.avg$far, (performance.c2.avg$detected*performance.c2.avg$prevented))

# Plot ROC curves
par(mfrow=c(1,2))
plot(performance.c2.avg$far, performance.c2.avg$detected, type='s', 
     xlab='False Positive Rate', ylab="Sensitivity", xlim=c(0,1))
plot(performance.c2.avg$far, performance.c2.avg$detected*performance.c2.avg$prevented, type='s', 
     xlab='False Positive Rate', ylab="Sensitivity x Prevented", xlim=c(0,1))
par(mfrow=c(1,1))
```

**Q3. Determine the effect of the gap parameter on the performance of the C2 algorithm (sensitivity, specificity, and detection delay). Vary the gap parameter over at least five settings and summarize your results.**

```{r q3}
# Place any additional code here
```

The results show that regardless of the size of the guard band, the ROC curves which are penalized for not predicting an outbreak in the simulation (right hand plots), have attenuated curves and in all cases the AUC declined by approximately 20% (see Table). Visually speaking, the ROC curves in the penalized algorithms are particularly attenuated in the upper left quadrant. This means that an algorithm's sensitivity and specificity is impacted by this timeliness penalty, and so its performance shifts towards lower sensitivities and higher false alarm rates, which is reflected in the reduced AUC. All of the ROC curves still attain high levels of sensitivity, but they do so at higher false alarm rates.

Varying the size of the guard band also has an effect on the curves. Having a longer gap between the index day and the calculation of the baseline attenuates both curves (penalized and non) particularly in the upper left quadrant. Sensitivity for the non-penalized ROC curves with 1 and 2 day gaps plateaued to 100% at much lower false positive rates (approximately 0.25) than the ROC curves with 5 or 7 days days, which plateaued at false positive rates of nearly 0.4. In terms of the penalized curves, the shift by gap size was much smaller, but qualitatively speaking a smaller gap seems to increase the sensitivity at lower false positive rates, and having a longer gap period attenuates this performance. 

### Apply Poisson Algorithm

```{r poisson}
# Apply Poisson algorithm to runs
res.p = lapply(runs, poisson_all, dow=FALSE, gap=2, window=56, interval=14, threshold=0.05)
# Determine detection and timeliness for each run
res.p.far = mapply(a.far, res.p, outbreaks)
res.p.detect = mapply(o.detected, res.p, outbreaks)
res.p.prevent = mapply(o.prevented, res.p, outbreaks)

performance.p.all = a.performance.all(res.p, outbreaks, n.cutoffs)
performance.p.avg = a.performance.avg(performance.p.all)

par(mfrow=c(1,2))
plot(performance.p.avg$far, performance.p.avg$detected, type='s', 
     xlab='False Positive Rate', ylab="Sensitivity", xlim=c(0,1))
plot(performance.p.avg$far, performance.p.avg$detected*performance.p.avg$prevented, type='s', 
     xlab='False Positive Rate', ylab="Sensitivity x Prevented", xlim=c(0,1))
par(mfrow=c(1,1))

auc.p = auc(performance.p.avg$far, performance.p.avg$detected)
auc.p.weighted = auc(performance.p.avg$far, (performance.p.avg$detected*performance.p.avg$prevented))
```


**Q4. Determine the effect of the dow parameter on the performance of the Poisson algorithm (sensitivity, specificity, and detection delay).**

```{r q4}
# Place any additional code here
```

The resulting ROC curves show that while the Poisson algorithm not accounting for day-of-the-week attains 100% sensitivity at relatively low false positive rates when un-penalized, when the algorithm is penalized for timeliness of detection, the ROC curve is significantly attenuated and does not reach above 80% until a very high false positive rate. This implies that the algorithm either fails to detect an outbreak, or does not detect an aberration until some time after the outbreak starts. Interestingly, the AUC is similar to the C2 algorithm AUCs, although the ROC curve itself is very differently shaped than that from the C2 algorithm. Overall, the Poisson algorithm does not perform as well as the C2 algorithm when it is enalized for timeliness, because it has lower sensitivities at tghe same false positive rates.
